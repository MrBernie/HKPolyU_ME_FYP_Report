@misc{devin_sound_2024,
  author = {Devin Pi},
  title = {Trusted Sound Source Localization},
  year = {2024},
  howpublished = {\url{https://github.com/Devin-Pi/trusted-sound-source-localization}},
  note = {Accessed: 2024-11-16}
}

@article{andrew_assessment_1993,
title = {Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems},
journal = {Speech Communication},
volume = {12},
number = {3},
pages = {247-251},
year = {1993},
issn = {0167-6393},
author = {Andrew Varga and Herman J.M. Steeneken},
keywords = {Speech recognizer assessment, calibrated data base, additive noise},
abstract = {The NOISEX-92 experiment and database is described and discussed. NOISEX-92 specifies a carefully controlled experiment on artificially noisy speech data, examining performance for a limited digit recognition task but with a relatively wide range of noises and signal-to-noise ratios. Example recognition results are given.
Zusammenfassung
Es werden die Datenbank und die Tests NOISEX-92 beschrieben und kommentiert. NOISEX-92 spezifiziert Tests über künstliche Lärmdaten, wobei die Leistungen im Rahmen der Erjennung von Zahlen und ein relativ breiter Rauschabstand bewertet werden. Es werden Beispiele der Ergebnisse für Erkennungen beschrieben.
Résumé
La base de données et les tests NOISEX-92 sont décrits et commentés. NOISEX-92 spécifie des tests sur des données bruitées artificiellement en évaluant les performances dans le cadre de la reconnaissance de chiffres et une gamme relativement large de rapports signal sur bruit. Des examples de scores de reconnaissances sont présentés.}
}

@INPROCEEDINGS{panayotov_resource_2015,
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Librispeech: An ASR corpus based on public domain audio books}, 
  year={2015},
  volume={},
  number={},
  pages={5206-5210},
  keywords={Resource description framework;Genomics;Bioinformatics;Blogs;Information services;Electronic publishing;Speech Recognition;Corpus;LibriVox},
  doi={10.1109/ICASSP.2015.7178964}}


@misc{yang_srp-dnn_2022,
      title={SRP-DNN: Learning Direct-Path Phase Difference for Multiple Moving Sound Source Localization}, 
      author={Bing Yang and Hong Liu and Xiaofei Li},
      year={2022},
      eprint={2202.07859},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
}

@article{noh_dereverberation_2020,
  author = {Noh, K. J. and Chang, J.},
  title = {Joint optimization of deep neural network-based dereverberation and beamforming for sound event detection in multi-channel environments},
  journal = {Sensors},
  year = {2020},
  volume = {20},
  issue = {7},
  pages = {1883},
  doi = {10.3390/s20071883}
}

@article{comminiello_beamforming_2013,
  author = {Comminiello, D. and Scarpiniti, M. and Parisi, R. and Uncini, A.},
  title = {Combined adaptive beamforming techniques for noise reduction in changing environments},
  journal = {2013 36th International Conference on Telecommunications and Signal Processing (TSP)},
  year = {2013},
}

@article{tachioka_method_2017,
      author = {Tachioka, Y. and Narita, T.},
      title = {Template-based method for compensation of time difference of arrival in passive sound source localization under reverberant and noisy environments},
      journal = {Journal of Signal Processing},
      year = {2017},
      volume = {21},
      issue = {2},
      pages = {73-79},
}

@article{hao_network_2020,
      author = {Hao, Y. and Kucuk, A. and Ganguly, A. and Panahi, I.},
      title = {Spectral flux-based convolutional neural network architecture for speech source localization and its real-time implementation},
      journal = {IEEE Access},
      year = {2020},
      volume = {8},
}

@article{mesaros_sound_2021,
	title = {Sound {Event} {Detection}: {A} tutorial},
	volume = {38},
	issn = {1558-0792},
	shorttitle = {Sound {Event} {Detection}},
	doi = {10.1109/MSP.2021.3090678},
	abstract = {Imagine standing on a street corner in the city. With your eyes closed you can hear and recognize a succession of sounds: cars passing by, people speaking, their footsteps when they walk by, and the continuous falling of rain. The recognition of all these sounds and interpretation of the perceived scene as a city street soundscape comes naturally to humans. It is, however, the result of years of "training": encountering and learning associations among the vast varieties of sounds in everyday life, the sources producing these sounds, and the names given to them.},
	number = {5},
	urldate = {2024-09-27},
	journal = {IEEE Signal Processing Magazine},
	author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas and Plumbley, Mark D.},
	month = sep,
	year = {2021},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Acoustics, Tutorials, Urban areas},
	pages = {67--83},
}

@article{desai_review_2022,
	title = {A {Review} on {Sound} {Source} {Localization} {Systems}},
	volume = {29},
	issn = {1886-1784},
	doi = {10.1007/s11831-022-09747-2},
	abstract = {Sound Source Localization (SSL) systems focus on finding the direction of a sound source. Sound source localization is an essential feature in robots and humanoids. Research is being done for two decades to optimize SSL techniques and enhance their accuracy. Presented in this review we have categorized various proposed SSL techniques into four main types. Out of which one type is SSL that is based on conventional algorithms like Generalized Cross Correlation (GCC), Multiple Signal Classification (MUSIC), Time Difference of Arrival (TDOA), etc. using multiple microphones array configurations. The second type involves techniques based on binaural signal processing using conventional algorithms (GCC, MUSIC, TDOA). SSL techniques that fall under the third and fourth types of categories are developed recently in the last decade with the rise of Convolutional Neural Network (CNN) algorithms. The third type of SSL technique makes use of multiple microphone array configurations using CNN and the fourth type involves the most recently evolved technique based on binaural signal processing using CNN. The different SSL techniques based on multiaural and binaural signals using the conventional algorithm as well as CNN are presented in this review. The review paper provides an overview of SSL systems in terms of the number of microphones used, layouts of microphonic arrays, algorithms to perform SSL and localization in 3D space (azimuth, elevation and distance). From the review we found that out of all SSL techniques, CNN is the emerging and optimized one. By using CNN in SSL systems, the least error rate of 0.1 \% was achieved.},
	language = {en},
	number = {7},
	urldate = {2024-09-27},
	journal = {Archives of Computational Methods in Engineering},
	author = {Desai, Dhwani and Mehendale, Ninad},
	month = nov,
	year = {2022},
	keywords = {Artificial Intelligence},
	pages = {4631--4642},
}

@article{knapp_generalized_1976,
	title = {The generalized correlation method for estimation of time delay},
	volume = {24},
	issn = {0096-3518},
	abstract = {A maximum likelihood (ML) estimator is developed for determining time delay between signals received at two spatially separated sensors in the presence of uncorrelated noise. This ML estimator can be realized as a pair of receiver prefilters followed by a cross correlator. The time argument at which the correlator achieves a maximum is the delay estimate. The ML estimator is compared with several other proposed processors of similar form. Under certain conditions the ML estimator is shown to be identical to one proposed by Hannan and Thomson [10] and MacDonald and Schultheiss [21]. Qualitatively, the role of the prefilters is to accentuate the signal passed to the correlator at frequencies for which the signal-to-noise (S/N) ratio is highest and, simultaneously, to suppress the noise power. The same type of prefiltering is provided by the generalized Eckart filter, which maximizes the S/N ratio of the correlator output. For low S/N ratio, the ML estimator is shown to be equivalent to Eckart prefiltering.},
	number = {4},
	urldate = {2024-09-27},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Knapp, C. and Carter, G.},
	month = aug,
	year = {1976},
	note = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing},
	keywords = {Frequency, Correlation, Correlators, Delay effects, Delay estimation, Mathematical model, Maximum likelihood estimation, Propagation delay, Signal processing, Signal to noise ratio},
	pages = {320--327},
}

@article{schmidt_multiple_1986,
	title = {Multiple emitter location and signal parameter estimation},
	volume = {34},
	issn = {1558-2221},
	doi = {10.1109/TAP.1986.1143830},
	abstract = {Processing the signals received on an array of sensors for the location of the emitter is of great enough interest to have been treated under many special case assumptions. The general problem considers sensors with arbitrary locations and arbitrary directional characteristics (gain/phase/polarization) in a noise/interference environment of arbitrary covariance matrix. This report is concerned first with the multiple emitter aspect of this problem and second with the generality of solution. A description is given of the multiple signal classification (MUSIC) algorithm, which provides asymptotically unbiased estimates of 1) number of incident wavefronts present; 2) directions of arrival (DOA) (or emitter locations); 3) strengths and cross correlations among the incident waveforms; 4) noise/interference strength. Examples and comparisons with methods based on maximum likelihood (ML) and maximum entropy (ME), as well as conventional beamforming are included. An example of its use as a multiple frequency estimator operating on time series is included.},
	number = {3},
	urldate = {2024-09-27},
	journal = {IEEE Transactions on Antennas and Propagation},
	author = {Schmidt, R.},
	month = mar,
	year = {1986},
	note = {Conference Name: IEEE Transactions on Antennas and Propagation},
	keywords = {Signal processing, Direction of arrival estimation, Frequency estimation, Interference, Multiple signal classification, Parameter estimation, Polarization, Sensor arrays, Sensor phenomena and characterization, Working environment noise},
	pages = {276--280},
}

@article{van_veen_beamforming_1988,
	title = {Beamforming: a versatile approach to spatial filtering},
	volume = {5},
	issn = {1558-1284},
	shorttitle = {Beamforming},
	doi = {10.1109/53.665},
	abstract = {An overview of beamforming from a signal-processing perspective is provided, with an emphasis on recent research. Data-independent, statistically optimum, adaptive, and partially adaptive beamforming are discussed. Basic notation, terminology, and concepts are included. Several beamformer implementations are briefly described.{\textless}{\textgreater}},
	number = {2},
	urldate = {2024-09-27},
	journal = {IEEE ASSP Magazine},
	author = {Van Veen, B.D. and Buckley, K.M.},
	month = apr,
	year = {1988},
	note = {Conference Name: IEEE ASSP Magazine},
	keywords = {Frequency, Interference, Sensor arrays, Apertures, Array signal processing, Feeds, Filtering, Microwave filters, Sampling methods, Spatial filters},
	pages = {4--24},
}

@article{xu_high-accuracy_2013,
	title = {High-{Accuracy} {TDOA}-{Based} {Localization} without {Time} {Synchronization}},
	volume = {24},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2012.248},
	abstract = {Localization is of great importance in mobile and wireless network applications. Time Difference of Arrival (TDOA) is one of the widely used localization schemes, in which the target (source) emits a signal and a number of anchors (receivers) record the arriving time of the source signal. By calculating the time difference of different receivers, the location of the target is estimated. In such a scheme, receivers must be precisely time synchronized. But time synchronization adds computational cost, and brings errors which may lower localization accuracy. Previous studies have shown that existing time synchronization approaches using low-cost devices are insufficiently accurate, or even infeasible under high requirement for accuracy. In our scheme (called Whistle), several asynchronous receivers record a target signal and a successive signal that is generated artificially. By two-signal sensing and sample counting techniques, time synchronization requirement can be removed, while high time resolution can be achieved. This design fundamentally changes TDOA in the sense of releasing the synchronization requirement and avoiding many sources of errors caused by time synchronization. We implement Whistle on commercial off-the-shelf (COTS) cell phones with acoustic signal and perform simulations with UWB signal. Especially we use Whistle to localize nodes of large-scale wireless networks, and also achieve desirable results. The extensive real-world experiments and simulations show that Whistle can be widely used with good accuracy.},
	number = {8},
	urldate = {2024-09-28},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Xu, Bin and Sun, Guodong and Yu, Ran and Yang, Zheng},
	month = aug,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {Acoustics, Accuracy, Clocks, Equations, Localization, Microphones, Receivers, Synchronization, TDOA, time synchronization, wireless application},
	pages = {1567--1576},
}

@inproceedings{kwon_analysis_2010,
	title = {Analysis of the {GCC}-{PHAT} technique for multiple sources},
	doi = {10.1109/ICCAS.2010.5670137},
	abstract = {Techniques to estimate the time delay of arrival (TDOA) using the measurements of the acoustic signals by microphones have been studied in various fields such as the robot auditory system, teleconference system and speech recognition system. One common method of determining TDOA is to compute the cross correlation function. The Generalized Cross Correlation (GCC) method, which calculates the correlation function by using inverse Fourier transformation of the cross power spectral density function multiplied by the proper weighting function, was proposed by Knapp and Cater in 1976. This method analyzed the weighting functions to estimate the optimal TDOA for a single source. In this paper, we derived the cross correlation function by GCC method with PHAT weighting function for multiple sources and ascertained the relationship between the correlation value and source characteristics. Moreover, we compared the derived GCC function for two sources case with the real GCC function calculated by the actual signals and verified their similarity.},
	urldate = {2024-09-28},
	booktitle = {{ICCAS} 2010},
	author = {Kwon, Byoungho and Park, Youngjin and Park, Youn-sik},
	month = oct,
	year = {2010},
	keywords = {Correlation, Mathematical model, Microphones, generalized cross correlation (GCC), multiple sources localization, Speech, time delay of arrival (TDOA), Vibration control, White noise},
	pages = {2070--2073},
}

@article{liu_arbitrary_2019,
	title = {Arbitrary {Microphone} {Array} {Optimization} {Method} {Based} on {TDOA} for {Specific} {Localization} {Scenarios}},
	volume = {19},
	issn = {1424-8220},
	doi = {10.3390/s19194326},
	abstract = {Various microphone array geometries (e.g., linear, circular, square, cubic, spherical, etc.) have been used to improve the positioning accuracy of sound source localization. However, whether these array structures are optimal for various specific localization scenarios is still a subject of debate. This paper addresses a microphone array optimization method for sound source localization based on TDOA (time difference of arrival). The geometric structure of the microphone array is established in parametric form. A triangulation method with TDOA was used to build the spatial sound source location model, which consists of a group of nonlinear multivariate equations. Through reasonable transformation, the nonlinear multivariate equations can be converted to a group of linear equations that can be approximately solved by the weighted least square method. Then, an optimization model based on particle swarm optimization (PSO) algorithm was constructed to optimize the geometric parameters of the microphone array under different localization scenarios combined with the spatial sound source localization model. In the optimization model, a reasonable fitness evaluation function is established which can comprehensively consider the positioning accuracy and robustness of the microphone array. In order to verify the array optimization method, two specific localization scenarios and two array optimization strategies for each localization scenario were constructed. The optimal array structure parameters were obtained through numerical iteration simulation. The localization performance of the optimal array structures obtained by the method proposed in this paper was compared with the optimal structures proposed in the literature as well as with random array structures. The simulation results show that the optimized array structure gave better positioning accuracy and robustness under both specific localization scenarios. The optimization model proposed could solve the problem of array geometric structure design based on TDOA and could achieve the customization of microphone array structures under different specific localization scenarios.},
	language = {en},
	number = {19},
	urldate = {2024-09-30},
	journal = {Sensors},
	author = {Liu, Haitao and Kirubarajan, Thia and Xiao, Qian},
	month = jan,
	year = {2019},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {TDOA, array optimization, PSO, source localization},
	pages = {4326},
}

@inproceedings{carevic_detection_2007,
	address = {Melbourne, Australia},
	title = {Detection and {Tracking} of {Underwater} {Targets} {Using} {Directional} {Sensors}},
	isbn = {978-1-4244-1501-4},
	doi = {10.1109/ISSNIP.2007.4496834},
	urldate = {2024-09-30},
	booktitle = {2007 3rd {International} {Conference} on {Intelligent} {Sensors}, {Sensor} {Networks} and {Information}},
	publisher = {IEEE},
	author = {Carevic, Dragana},
	year = {2007},
	pages = {143--148},
}

@article{chung_sound_2022,
	title = {Sound {Localization} {Based} on {Acoustic} {Source} {Using} {Multiple} {Microphone} {Array} in an {Indoor} {Environment}},
	volume = {11},
	issn = {2079-9292},
	doi = {10.3390/electronics11060890},
	abstract = {Sound signals have been widely applied in various fields. One of the popular applications is sound localization, where the location and direction of a sound source are determined by analyzing the sound signal. In this study, two microphone linear arrays were used to locate the sound source in an indoor environment. The TDOA is also designed to deal with the problem of delay in the reception of sound signals from two microphone arrays by using the generalized cross-correlation algorithm to calculate the TDOA. The proposed microphone array system with the algorithm can successfully estimate the sound source’s location. The test was performed in a standardized chamber. This experiment used two microphone arrays, each with two microphones. The experimental results prove that the proposed method can detect the sound source and obtain good performance with a position error of about 2.0{\textasciitilde}2.3 cm and angle error of about 0.74 degrees. Therefore, the experimental results demonstrate the feasibility of the system.},
	language = {en},
	number = {6},
	urldate = {2024-09-30},
	journal = {Electronics},
	author = {Chung, Ming-An and Chou, Hung-Chi and Lin, Chia-Wei},
	month = mar,
	year = {2022},
	pages = {890},
}

@inproceedings{valin_localization_2004,
	address = {New Orleans, LA, USA},
	title = {Localization of simultaneous moving sound sources for mobile robot using a frequency- domain steered beamformer approach},
	isbn = {978-0-7803-8232-9},
	doi = {10.1109/ROBOT.2004.1307286},
	urldate = {2024-09-30},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation}, 2004. {Proceedings}. {ICRA} '04. 2004},
	publisher = {IEEE},
	author = {Valin, J.-M. and Michaud, F. and Hadjou, B. and Rouat, J.},
	year = {2004},
	pages = {1033--1038 Vol.1},
}

@article{parra_geometric_2002,
	title = {Geometric source separation: merging convolutive source separation with geometric beamforming},
	volume = {10},
	issn = {1063-6676},
	shorttitle = {Geometric source separation},
	doi = {10.1109/TSA.2002.803443},
	language = {en},
	number = {6},
	urldate = {2024-09-30},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Parra, L.C. and Alvino, C.V.},
	month = sep,
	year = {2002},
	pages = {352--362},
}

@inproceedings{mao_mfcc_2017,
	address = {Xi'an, China},
	title = {{MFCC} combined with sparse coding for sound event classification under different noise environments},
	isbn = {978-94-6252-320-3},
	doi = {10.2991/eeeis-16.2017.50},
	language = {en},
	urldate = {2024-09-30},
	booktitle = {Proceedings of the 2nd {Annual} {International} {Conference} on {Electronics}, {Electrical} {Engineering} and {Information} {Science} ({EEEIS} 2016)},
	publisher = {Atlantis Press},
	author = {Mao, Jia-Min and Wu, Yun-Peng and Liu, Li-Yang and Li, Wei-Feng},
	year = {2017},
}

@inproceedings{gubka_elementary_2013,
	address = {Pardubice},
	title = {Elementary sound based audio pattern searching},
	isbn = {978-1-4673-5519-3 978-1-4673-5516-2 978-1-4673-5518-6},
	doi = {10.1109/RadioElek.2013.6530940},
	urldate = {2024-09-30},
	booktitle = {2013 23rd {International} {Conference} {Radioelektronika} ({RADIOELEKTRONIKA})},
	publisher = {IEEE},
	author = {Gubka, R. and Kuba, M.},
	month = apr,
	year = {2013},
	pages = {325--328},
}

@article{lopatka_detection_2016,
	title = {Detection, classification and localization of acoustic events in the presence of background noise for acoustic surveillance of hazardous situations},
	volume = {75},
	issn = {1380-7501, 1573-7721},
	doi = {10.1007/s11042-015-3105-4},
	language = {en},
	number = {17},
	urldate = {2024-09-30},
	journal = {Multimedia Tools and Applications},
	author = {Lopatka, K. and Kotus, J. and Czyzewski, A.},
	month = sep,
	year = {2016},
	pages = {10407--10439},
}

@article{wang_deep-learning-assisted_2022,
	title = {Deep-{Learning}-{Assisted} {Sound} {Source} {Localization} {From} a {Flying} {Drone}},
	volume = {22},
	issn = {1530-437X, 1558-1748, 2379-9153},
	doi = {10.1109/JSEN.2022.3207660},
	number = {21},
	urldate = {2024-09-30},
	journal = {IEEE Sensors Journal},
	author = {Wang, Lin and Cavallaro, Andrea},
	month = nov,
	year = {2022},
	pages = {20828--20838},
}

@article{zhang_deep_2017,
	title = {Deep {Learning} {Based} {Binaural} {Speech} {Separation} in {Reverberant} {Environments}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	doi = {10.1109/TASLP.2017.2687104},
	number = {5},
	urldate = {2024-09-30},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Zhang, Xueliang and Wang, DeLiang},
	month = may,
	year = {2017},
	pages = {1075--1084},
}

@article{songgong_acoustic_2022,
	title = {Acoustic {Source} {Localization} in the {Circular} {Harmonic} {Domain} {Using} {Deep} {Learning} {Architecture}},
	volume = {30},
	issn = {2329-9290, 2329-9304},
	doi = {10.1109/TASLP.2022.3190723},
	urldate = {2024-09-30},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {SongGong, Kunkun and Wang, Wenwu and Chen, Huawei},
	year = {2022},
	pages = {2475--2491},
}

@inproceedings{moing_learning_2019,
	address = {Kuala Lumpur, Malaysia},
	title = {Learning {Multiple} {Sound} {Source} {2D} {Localization}},
	isbn = {978-1-72811-817-8},
	doi = {10.1109/MMSP.2019.8901685},
	urldate = {2024-09-30},
	booktitle = {2019 {IEEE} 21st {International} {Workshop} on {Multimedia} {Signal} {Processing} ({MMSP})},
	publisher = {IEEE},
	author = {Moing, Guillaume Le and Vinayavekhin, Phongtharin and Inoue, Tadanobu and Vongkulbhisal, Jayakorn and Munawar, Asim and Tachibana, Ryuki and Agravante, Don Joven},
	month = sep,
	year = {2019},
	pages = {1--6},
}

@inproceedings{adavanne_direction_2018,
	address = {Rome},
	title = {Direction of {Arrival} {Estimation} for {Multiple} {Sound} {Sources} {Using} {Convolutional} {Recurrent} {Neural} {Network}},
	isbn = {978-90-827970-1-5},
	doi = {10.23919/EUSIPCO.2018.8553182},
	urldate = {2024-09-30},
	booktitle = {2018 26th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {IEEE},
	author = {Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas},
	month = sep,
	year = {2018},
	pages = {1462--1466},
}

@inproceedings{wang_robust_2018,
  title={Robust TDOA Estimation Based on Time-Frequency Masking and Deep Neural Networks},
  author={Zhong-Qiu Wang and Xueliang Zhang and Deliang Wang},
  booktitle={Interspeech},
  year={2018},
}

@article{lee_sound_2020,
	title = {Sound {Source} {Localization} {Based} on {GCC}-{PHAT} {With} {Diffuseness} {Mask} in {Noisy} and {Reverberant} {Environments}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2963768},
	urldate = {2024-09-30},
	journal = {IEEE Access},
	author = {Lee, Ran and Kang, Min-Seok and Kim, Bo-Hyun and Park, Kang-Ho and Lee, Sung Q and Park, Hyung-Min},
	year = {2020},
	pages = {7373--7382},
}

@article{ma_exploiting_2017,
	title = {Exploiting {Deep} {Neural} {Networks} and {Head} {Movements} for {Robust} {Binaural} {Localization} of {Multiple} {Sources} in {Reverberant} {Environments}},
	volume = {25},
	issn = {2329-9290, 2329-9304},
	doi = {10.1109/TASLP.2017.2750760},
	number = {12},
	urldate = {2024-09-30},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Ma, Ning and May, Tobias and Brown, Guy J.},
	month = dec,
	year = {2017},
	pages = {2444--2453},
}

@inproceedings{wang_visualized_2013,
	address = {Xi'an, China},
	title = {A visualized acoustic saliency feature extraction method for environment sound signal processing},
	isbn = {978-1-4799-2827-9 978-1-4799-2825-5},
	doi = {10.1109/TENCON.2013.6718918},
	urldate = {2024-09-30},
	booktitle = {2013 {IEEE} {International} {Conference} of {IEEE} {Region} 10 ({TENCON} 2013)},
	publisher = {IEEE},
	author = {Wang, Jingyu and Zhang, Ke and Madani, Kurosh and Sabourin, Christophe},
	month = oct,
	year = {2013},
	pages = {1--4},
}

@article{di_applicability_2023,
	title = {Applicability of {VGGish} embedding in bee colony monitoring: comparison with {MFCC} in colony sound classification},
	volume = {11},
	issn = {2167-8359},
	shorttitle = {Applicability of {VGGish} embedding in bee colony monitoring},
	doi = {10.7717/peerj.14696},
	abstract = {Background
              Bee colony sound is a continuous, low-frequency buzzing sound that varies with the environment or the colony’s behavior and is considered meaningful. Bees use sounds to communicate within the hive, and bee colony sounds investigation can reveal helpful information about the circumstances in the colony. Therefore, one crucial step in analyzing bee colony sounds is to extract appropriate acoustic feature.
            
            

              Methods
              This article uses VGGish (a visual geometry group—like audio classification model) embedding and Mel-frequency Cepstral Coefficient (MFCC) generated from three bee colony sound datasets, to train four machine learning algorithms to determine which acoustic feature performs better in bee colony sound recognition.


​            

              Results
              The results showed that VGGish embedding performs better than or on par with MFCC in all three datasets.},
    language = {en},
    urldate = {2024-09-30},
    journal = {PeerJ},
    author = {Di, Nayan and Sharif, Muhammad Zahid and Hu, Zongwen and Xue, Renjie and Yu, Baizhong},
    month = jan,
    year = {2023},
    pages = {e14696},

}

@misc{nsalo_kong_underwater_2024,
	title = {Underwater {Acoustic} {Monitoring}: {A} {Comprehensive} {Approach} to {Enhance} {MFCC} {Robustness} and {Classification} {Accuracy}},
	shorttitle = {Underwater {Acoustic} {Monitoring}},
	doi = {10.20944/preprints202401.0652.v1},
	abstract = {Passive marine listening, involving the acoustic monitoring of underwater environments, has become increasingly vital for scientific research, environmental monitoring, and defense applications. However, the success of such applications critically depends on the ability to extract meaningful information from the often noisy and dynamic underwater acoustic environment. In this context, Mel-frequency cepstral coefficients (MFCCs) have emerged as an essential tool for feature extraction, capturing the spectral characteristics of marine sounds and making them ideal for species identification and sound event detection. This research presents an innovative approach to enhance the robustness of MFCC-based passive marine listening through adaptive noise reduction techniques. The proposed approach utilizes dynamic spectral subtraction to counter underwater noise, resulting in enhanced signal-to-noise ratios for desired sounds. This adaptive system adjusts to dynamic underwater noise, allowing it to concentrate on target sounds and suppress interference effectively. The experimentation further validates the effectiveness of the proposed approach, with results reaching 99\% on the full dataset of 570 mammals and vessels, demonstrating the effectiveness of SVM and Random Forests in the classification of underwater audio.},
	urldate = {2024-09-30},
	author = {Nsalo Kong, Darryl Franck and Shen, Chong and Tian, Chuan and Zhang, Sheng Rong},
	month = jan,
	year = {2024},
}

@article{palaniappan_comparative_2014,
	title = {A comparative study of the svm and k-nn machine learning algorithms for the diagnosis of respiratory pathologies using pulmonary acoustic signals},
	volume = {15},
	issn = {1471-2105},
	doi = {10.1186/1471-2105-15-223},
	language = {en},
	number = {1},
	urldate = {2024-09-30},
	journal = {BMC Bioinformatics},
	author = {Palaniappan, Rajkumar and Sundaraj, Kenneth and Sundaraj, Sebastian},
	month = dec,
	year = {2014},
	pages = {223},
}

@article{wang_research_2024,
	title = {Research on fault type identification method for electrical equipment based on {MFCC}-{CNN}},
	volume = {2770},
	issn = {1742-6588, 1742-6596},
	doi = {10.1088/1742-6596/2770/1/012028},
	abstract = {Abstract
            As a critical component of the power system, the secure and reliable functioning of electrical equipment is the key to ensuring the dependability of energy supply. In this article, a fault type identification method based on Mel-Frequency Cepstral Coefficients-based Convolutional Neural Networks (MFCC-CNN) for electrical equipment was proposed. Firstly, the reasons for the sound generated by electrical equipment during the operation were analyzed. Then the MFCC coefficient of sound features was extracted by collecting and processing the faulty sound of electrical equipment. Finally, a CNN model was established to train and recognize sound signals. This method combines the recognition and classification of sound with deep learning (DL), which can significantly improve the efficiency of fault diagnosis.},
	number = {1},
	urldate = {2024-09-30},
	journal = {Journal of Physics: Conference Series},
	author = {Wang, Chao and Zheng, Haotian and Yin, Qing and Yi, Xin},
	month = may,
	year = {2024},
	pages = {012028},
}

@article{tan_sound_2021,
	title = {Sound {Source} {Localization} {Using} a {Convolutional} {Neural} {Network} and {Regression} {Model}},
	volume = {21},
	issn = {1424-8220},
	doi = {10.3390/s21238031},
	abstract = {In this research, a novel sound source localization model is introduced that integrates a convolutional neural network with a regression model (CNN-R) to estimate the sound source angle and distance based on the acoustic characteristics of the interaural phase difference (IPD). The IPD features of the sound signal are firstly extracted from time-frequency domain by short-time Fourier transform (STFT). Then, the IPD features map is fed to the CNN-R model as an image for sound source localization. The Pyroomacoustics platform and the multichannel impulse response database (MIRD) are used to generate both simulated and real room impulse response (RIR) datasets. The experimental results show that an average accuracy of 98.96\% and 98.31\% are achieved by the proposed CNN-R for angle and distance estimations in the simulation scenario at SNR = 30 dB and RT60 = 0.16 s, respectively. Moreover, in the real environment, the average accuracies of the angle and distance estimations are 99.85\% and 99.38\% at SNR = 30 dB and RT60 = 0.16 s, respectively. The performance obtained in both scenarios is superior to that of existing models, indicating the potential of the proposed CNN-R model for real-life applications.},
	language = {en},
	number = {23},
	urldate = {2024-09-30},
	journal = {Sensors},
	author = {Tan, Tan-Hsu and Lin, Yu-Tang and Chang, Yang-Lang and Alkhaleefah, Mohammad},
	month = dec,
	year = {2021},
	pages = {8031},
}

@article{chakrabarty_multi-speaker_2019,
	title = {Multi-{Speaker} {DOA} {Estimation} {Using} {Deep} {Convolutional} {Networks} {Trained} {With} {Noise} {Signals}},
	volume = {13},
	issn = {1932-4553, 1941-0484},
	doi = {10.1109/JSTSP.2019.2901664},
	number = {1},
	urldate = {2024-09-30},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Chakrabarty, Soumitro and Habets, Emanuel A. P.},
	month = mar,
	year = {2019},
	pages = {8--21},
}


@article{pujol_beamlearning_2021,
	title = {{BeamLearning}: {An} end-to-end deep learning approach for the angular localization of sound sources using raw multichannel acoustic pressure data},
	volume = {149},
	issn = {0001-4966, 1520-8524},
	shorttitle = {{BeamLearning}},
	doi = {10.1121/10.0005046},
	abstract = {Sound source localization using multichannel signal processing has been a subject of active research for decades. In recent years, the use of deep learning in audio signal processing has significantly improved the performances for machine hearing. This has motivated the scientific community to also develop machine learning strategies for source localization applications. This paper presents BeamLearning, a multiresolution deep learning approach that allows the encoding of relevant information contained in unprocessed time-domain acoustic signals captured by microphone arrays. The use of raw data aims at avoiding the simplifying hypothesis that most traditional model-based localization methods rely on. Benefits of its use are shown for real-time sound source two-dimensional localization tasks in reverberating and noisy environments. Since supervised machine learning approaches require large-sized, physically realistic, precisely labelled datasets, a fast graphics processing unit-based computation of room impulse responses was developed using fractional delays for image source models. A thorough analysis of the network representation and extensive performance tests are carried out using the BeamLearning network with synthetic and experimental datasets. Obtained results demonstrate that the BeamLearning approach significantly outperforms the wideband MUSIC and steered response power-phase transform methods in terms of localization accuracy and computational efficiency in the presence of heavy measurement noise and reverberation.},
	language = {en},
	number = {6},
	urldate = {2024-09-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Pujol, Hadrien and Bavu, Éric and Garcia, Alexandre},
	month = jun,
	year = {2021},
	pages = {4248--4263},
}

@misc{schymura_pilot_2021,
	title = {{PILOT}: {Introducing} {Transformers} for {Probabilistic} {Sound} {Event} {Localization}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{PILOT}},
	doi = {10.48550/ARXIV.2106.03903},
	abstract = {Sound event localization aims at estimating the positions of sound sources in the environment with respect to an acoustic receiver (e.g. a microphone array). Recent advances in this domain most prominently focused on utilizing deep recurrent neural networks. Inspired by the success of transformer architectures as a suitable alternative to classical recurrent neural networks, this paper introduces a novel transformer-based sound event localization framework, where temporal dependencies in the received multi-channel audio signals are captured via self-attention mechanisms. Additionally, the estimated sound event positions are represented as multivariate Gaussian variables, yielding an additional notion of uncertainty, which many previously proposed deep learning-based systems designed for this application do not provide. The framework is evaluated on three publicly available multi-source sound event localization datasets and compared against state-of-the-art methods in terms of localization error and event detection accuracy. It outperforms all competing systems on all datasets with statistical significant differences in performance.},
	urldate = {2024-09-30},
	publisher = {arXiv},
	author = {Schymura, Christopher and Bönninghoff, Benedikt and Ochiai, Tsubasa and Delcroix, Marc and Kinoshita, Keisuke and Nakatani, Tomohiro and Araki, Shoko and Kolossa, Dorothea},
	year = {2021},
	note = {Version Number: 1},
	keywords = {Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Machine Learning (cs.LG), Sound (cs.SD)},
	annote = {Other
Accepted at INTERSPEECH 2021},
}

@article{niu_deep-learning_2019,
	title = {Deep-learning source localization using multi-frequency magnitude-only data},
	volume = {146},
	issn = {0001-4966, 1520-8524},
	doi = {10.1121/1.5116016},
	abstract = {A deep learning approach based on big data is proposed to locate broadband acoustic sources using a single hydrophone in ocean waveguides with uncertain bottom parameters. Several 50-layer residual neural networks, trained on a huge number of sound field replicas generated by an acoustic propagation model, are used to handle the bottom uncertainty in source localization. A two-step training strategy is presented to improve the training of the deep models. First, the range is discretized in a coarse (5 km) grid. Subsequently, the source range within the selected interval and source depth are discretized on a finer (0.1 km and 2 m) grid. The deep learning methods were demonstrated for simulated magnitude-only multi-frequency data in uncertain environments. Experimental data from the China Yellow Sea also validated the approach.},
	language = {en},
	number = {1},
	urldate = {2024-09-30},
	journal = {The Journal of the Acoustical Society of America},
	author = {Niu, Haiqiang and Gong, Zaixiao and Ozanich, Emma and Gerstoft, Peter and Wang, Haibin and Li, Zhenglin},
	month = jul,
	year = {2019},
	pages = {211--222},
}

@article{liu_rolling_2016,
	title = {Rolling {Bearing} {Fault} {Diagnosis} {Based} on {STFT}-{Deep} {Learning} and {Sound} {Signals}},
	volume = {2016},
	issn = {1070-9622, 1875-9203},
	doi = {10.1155/2016/6127479},
	abstract = {The main challenge of fault diagnosis lies in finding good fault features. A deep learning network has the ability to automatically learn good characteristics from input data in an unsupervised fashion, and its unique layer-wise pretraining and fine-tuning using the backpropagation strategy can solve the difficulties of training deep multilayer networks. Stacked sparse autoencoders or other deep architectures have shown excellent performance in speech recognition, face recognition, text classification, image recognition, and other application domains. Thus far, however, there have been very few research studies on deep learning in fault diagnosis. In this paper, a new rolling bearing fault diagnosis method that is based on short-time Fourier transform and stacked sparse autoencoder is first proposed; this method analyzes sound signals. After spectrograms are obtained by short-time Fourier transform, stacked sparse autoencoder is employed to automatically extract the fault features, and softmax regression is adopted as the method for classifying the fault modes. The proposed method, when applied to sound signals that are obtained from a rolling bearing test rig, is compared with empirical mode decomposition, Teager energy operator, and stacked sparse autoencoder when using vibration signals to verify the performance and effectiveness of the proposed method.},
	language = {en},
	urldate = {2024-09-30},
	journal = {Shock and Vibration},
	author = {Liu, Hongmei and Li, Lianfeng and Ma, Jian},
	year = {2016},
	pages = {1--12},
}

@article{davila-chacon_enhanced_2019,
	title = {Enhanced {Robot} {Speech} {Recognition} {Using} {Biomimetic} {Binaural} {Sound} {Source} {Localization}},
	volume = {30},
	issn = {2162-237X, 2162-2388},
	doi = {10.1109/TNNLS.2018.2830119},
	number = {1},
	urldate = {2024-09-30},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Davila-Chacon, Jorge and Liu, Jindong and Wermter, Stefan},
	month = jan,
	year = {2019},
	pages = {138--150},
}

@article{chen_multiple_2022,
	title = {Multiple {Sound} {Source} {Localization}, {Separation}, and {Reconstruction} by {Microphone} {Array}: {A} {DNN}-{Based} {Approach}},
	volume = {12},
	issn = {2076-3417},
	shorttitle = {Multiple {Sound} {Source} {Localization}, {Separation}, and {Reconstruction} by {Microphone} {Array}},
	doi = {10.3390/app12073428},
	abstract = {Synchronistical localization, separation, and reconstruction for multiple sound sources are usually necessary in various situations, such as in conference rooms, living rooms, and supermarkets. To improve the intelligibility of speech signals, the application of deep neural networks (DNNs) has achieved considerable success in the area of time-domain signal separation and reconstruction. In this paper, we propose a hybrid microphone array signal processing approach for the nearfield scenario that combines the beamforming technique and DNN. Using this method, the challenge of identifying both the sound source location and content can be overcome. Moreover, the use of a sequenced virtual sound field reconstruction process enables the proposed approach to be quite suitable for a sound field which contains a dominant, stronger sound source and masked, weaker sound sources. Using this strategy, all traceable, mainly sound, sources can be discovered by loops in a given sound field. The operational duration and accuracy of localization are further improved by substituting the broadband weighted multiple signal classification (BW-MUSIC) method for the conventional delay-and-sum (DAS) beamforming algorithm. The effectiveness of the proposed method for localizing and reconstructing speech signals was validated by simulations and experiments with promising results. The localization results were accurate, while the similarity and correlation between the reconstructed and original signals was high.},
	language = {en},
	number = {7},
	urldate = {2024-09-30},
	journal = {Applied Sciences},
	author = {Chen, Long and Chen, Guitong and Huang, Lei and Choy, Yat-Sze and Sun, Weize},
	month = mar,
	year = {2022},
	pages = {3428},
}

@inproceedings{moing_learning_2019-1,
	address = {Kuala Lumpur, Malaysia},
	title = {Learning {Multiple} {Sound} {Source} {2D} {Localization}},
	isbn = {978-1-72811-817-8},
	doi = {10.1109/MMSP.2019.8901685},
	urldate = {2024-09-30},
	booktitle = {2019 {IEEE} 21st {International} {Workshop} on {Multimedia} {Signal} {Processing} ({MMSP})},
	publisher = {IEEE},
	author = {Moing, Guillaume Le and Vinayavekhin, Phongtharin and Inoue, Tadanobu and Vongkulbhisal, Jayakorn and Munawar, Asim and Tachibana, Ryuki and Agravante, Don Joven},
	month = sep,
	year = {2019},
	pages = {1--6},
}

@misc{adavanne_differentiable_2021,
	title = {Differentiable {Tracking}-{Based} {Training} of {Deep} {Learning} {Sound} {Source} {Localizers}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.2111.00030},
	abstract = {Data-based and learning-based sound source localization (SSL) has shown promising results in challenging conditions, and is commonly set as a classification or a regression problem. Regression-based approaches have certain advantages over classification-based, such as continuous direction-of-arrival estimation of static and moving sources. However, multi-source scenarios require multiple regressors without a clear training strategy up-to-date, that does not rely on auxiliary information such as simultaneous sound classification. We investigate end-to-end training of such methods with a technique recently proposed for video object detectors, adapted to the SSL setting. A differentiable network is constructed that can be plugged to the output of the localizer to solve the optimal assignment between predictions and references, optimizing directly the popular CLEAR-MOT tracking metrics. Results indicate large improvements over directly optimizing mean squared errors, in terms of localization error, detection metrics, and tracking capabilities.},
	urldate = {2024-09-30},
	publisher = {arXiv},
	author = {Adavanne, Sharath and Politis, Archontis and Virtanen, Tuomas},
	year = {2021},
	note = {Version Number: 1},
	keywords = {Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, Sound (cs.SD)},
	annote = {Other
Submitted to IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA2021)},
}

@inproceedings{luo_tasnet_2018,
	address = {Calgary, AB},
	title = {{TaSNet}: {Time}-{Domain} {Audio} {Separation} {Network} for {Real}-{Time}, {Single}-{Channel} {Speech} {Separation}},
	isbn = {978-1-5386-4658-8},
	shorttitle = {{TaSNet}},
	doi = {10.1109/ICASSP.2018.8462116},
	urldate = {2024-09-30},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Luo, Yi and Mesgarani, Nima},
	month = apr,
	year = {2018},
	pages = {696--700},
}

@article{adavanne_sound_2019,
	title = {Sound {Event} {Localization} and {Detection} of {Overlapping} {Sources} {Using} {Convolutional} {Recurrent} {Neural} {Networks}},
	volume = {13},
	issn = {1932-4553, 1941-0484},
	doi = {10.1109/JSTSP.2018.2885636},
	number = {1},
	urldate = {2024-09-30},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Adavanne, Sharath and Politis, Archontis and Nikunen, Joonas and Virtanen, Tuomas},
	month = mar,
	year = {2019},
	pages = {34--48},
}

@article{zhang_mtf-crnn_2020,
	title = {{MTF}-{CRNN}: {Multiscale} {Time}-{Frequency} {Convolutional} {Recurrent} {Neural} {Network} for {Sound} {Event} {Detection}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {{MTF}-{CRNN}},
	doi = {10.1109/ACCESS.2020.3015047},
	urldate = {2024-09-30},
	journal = {IEEE Access},
	author = {Zhang, Keming and Cai, Yuanwen and Ren, Yuan and Ye, Ruida and He, Liang},
	year = {2020},
	pages = {147337--147348},
}

@inproceedings{phan_robust_2016,
  title={Robust Audio Event Recognition with 1-Max Pooling Convolutional Neural Networks},
  author={Huy Phan and Lars Hertel and Marco Maass and Alfred Mertins},
  booktitle={Interspeech},
  year={2016},
}

@article{kim_multi-scale_2022,
	title = {Multi-{Scale} {Features} for {Transformer} {Model} to {Improve} the {Performance} of {Sound} {Event} {Detection}},
	volume = {12},
	issn = {2076-3417},
	doi = {10.3390/app12052626},
	abstract = {To alleviate the problem of performance degradation due to the varied sound durations of competing classes in sound event detection, we propose a method that utilizes multi-scale features for sound event detection. We employed a feature-pyramid component in a deep neural network architecture based on the Transformer encoder that is used to efficiently model the time correlation of sound signals because of its superiority over conventional recurrent neural networks, as demonstrated in recent studies. We used layers of convolutional neural networks to produce two-dimensional acoustic features that are input into the Transformer encoders. The outputs of the Transformer encoders at different levels of the network are combined to obtain the multi-scale features to feed the fully connected feed-forward neural network, which acts as the final classification layer. The proposed method is motivated by the idea that multi-scale features make the network more robust against the dynamic duration of the sound signals depending on their classes. We also applied the proposed method to a mean-teacher model, based on the Transformer encoder, to demonstrate its effectiveness on a large set of unlabeled data. We conducted experiments using the DCASE 2019 Task 4 dataset to evaluate the performance of the proposed method. The experimental results show that the proposed architecture outperforms the baseline network without multi-scale features.},
	language = {en},
	number = {5},
	urldate = {2024-09-30},
	journal = {Applied Sciences},
	author = {Kim, Soo-Jong and Chung, Yong-Joo},
	month = mar,
	year = {2022},
	pages = {2626},
}

@misc{drude_sms-wsj_2019,
	title = {{SMS}-{WSJ}: {Database}, performance measures, and baseline recipe for multi-channel source separation and recognition},
	shorttitle = {{SMS}-{WSJ}},
	abstract = {We present a multi-channel database of overlapping speech for training, evaluation, and detailed analysis of source separation and extraction algorithms: SMS-WSJ -- Spatialized Multi-Speaker Wall Street Journal. It consists of artificially mixed speech taken from the WSJ database, but unlike earlier databases we consider all WSJ0+1 utterances and take care of strictly separating the speaker sets present in the training, validation and test sets. When spatializing the data we ensure a high degree of randomness w.r.t. room size, array center and rotation, as well as speaker position. Furthermore, this paper offers a critical assessment of recently proposed measures of source separation performance. Alongside the code to generate the database we provide a source separation baseline and a Kaldi recipe with competitive word error rates to provide common ground for evaluation.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Drude, Lukas and Heitkaemper, Jens and Boeddeker, Christoph and Haeb-Umbach, Reinhold},
	month = oct,
	year = {2019},
	note = {arXiv:1910.13934 [cs]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
	annote = {Comment: Submitted to ICASSP 2020},
}

@inproceedings{strauss_dregon_2018,
	address = {Madrid},
	title = {{DREGON}: {Dataset} and {Methods} for {UAV}-{Embedded} {Sound} {Source} {Localization}},
	isbn = {978-1-5386-8094-0},
	shorttitle = {{DREGON}},
	doi = {10.1109/IROS.2018.8593581},
	abstract = {This paper introduces DREGON, a novel publiclyavailable dataset that aims at pushing research in sound source localization using a microphone array embedded in an unmanned aerial vehicle (UAV). The dataset contains both clean and noisy in-ﬂight audio recordings continuously annotated with the 3D position of the target sound source using an accurate motion capture system. In addition, various signals of interests are available such as the rotational speed of individual rotors and inertial measurements at all time. Besides introducing the dataset, this paper sheds light on the speciﬁc properties, challenges and opportunities brought by the emerging task of UAV-embedded sound source localization. Several baseline methods are evaluated and compared on the dataset, with real-time applicability in mind. Very promising results are obtained for the localization of a broad-band source in loud noise conditions, while speech localization remains a challenge under extreme noise levels.},
	language = {en},
	urldate = {2024-10-14},
	booktitle = {2018 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Strauss, Martin and Mordel, Pol and Miguet, Victor and Deleforge, Antoine},
	month = oct,
	year = {2018},
	pages = {1--8},
}

@misc{garofolo_john_s_timit_1993,
	title = {{TIMIT} {Acoustic}-{Phonetic} {Continuous} {Speech} {Corpus}},
	doi = {10.35111/17GK-BN40},
	abstract = {{\textless}h3{\textgreater}Introduction{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The TIMIT corpus includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16kHz speech waveform file for each utterance. Corpus design was a joint effort among the Massachusetts Institute of Technology (MIT), SRI International (SRI) and Texas Instruments, Inc. (TI). The speech was recorded at TI, transcribed at MIT and verified and prepared for CD-ROM production by the National Institute of Standards and Technology (NIST).{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}p{\textgreater}The TIMIT corpus transcriptions have been hand verified. Test and training subsets, balanced for phonetic and dialectal coverage, are specified. Tabular computer-searchable information is included as well as written documentation.{\textless}/p{\textgreater}{\textless}br{\textgreater} 
{\textless}h3{\textgreater}Samples{\textless}/h3{\textgreater}{\textless}br{\textgreater} 
{\textless}ul{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.phn" rel="nofollow"{\textgreater}phonemes{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.txt" rel="nofollow"{\textgreater}transcripts{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wav" rel="nofollow"{\textgreater}audio{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}li{\textgreater}{\textless}a href="desc/addenda/LDC93S1.wrd" rel="nofollow"{\textgreater}word list{\textless}/a{\textgreater}{\textless}/li{\textgreater}{\textless}br{\textgreater} 
{\textless}/ul{\textgreater}{\textless}/br{\textgreater} 
Portions © 1993 Trustees of the University of Pennsylvania},
	urldate = {2024-10-14},
	publisher = {Linguistic Data Consortium},
	author = {{Garofolo, John S.} and {Lamel, Lori F.} and {Fisher, William M.} and {Pallett, David S.} and {Dahlgren, Nancy L.} and {Zue, Victor} and {Fiscus, Jonathan G.}},
	year = {1993},
	note = {Artwork Size: 715776 KB
Pages: 715776 KB},
}

@article{varga_assessment_1993,
	title = {Assessment for automatic speech recognition: {II}. {NOISEX}-92: {A} database and an experiment to study the effect of additive noise on speech recognition systems},
	volume = {12},
	issn = {01676393},
	shorttitle = {Assessment for automatic speech recognition},
	doi = {10.1016/0167-6393(93)90095-3},
	language = {en},
	number = {3},
	urldate = {2024-10-14},
	journal = {Speech Communication},
	author = {Varga, Andrew and Steeneken, Herman J.M.},
	month = jul,
	year = {1993},
	pages = {247--251},
}

@article{diaz-guerra_gpurir_2021,
	title = {{gpuRIR}: {A} {Python} {Library} for {Room} {Impulse} {Response} {Simulation} with {GPU} {Acceleration}},
	volume = {80},
	issn = {1380-7501, 1573-7721},
	shorttitle = {{gpuRIR}},
	doi = {10.1007/s11042-020-09905-3},
	abstract = {The Image Source Method (ISM) is one of the most employed techniques to calculate acoustic Room Impulse Responses (RIRs), however, its computational complexity grows fast with the reverberation time of the room and its computation time can be prohibitive for some applications where a huge number of RIRs are needed. In this paper, we present a new implementation that dramatically improves the computation speed of the ISM by using Graphic Processing Units (GPUs) to parallelize both the simulation of multiple RIRs and the computation of the images inside each RIR. Additional speedups were achieved by exploiting the mixed precision capabilities of the newer GPUs and by using lookup tables. We provide a Python library under GNU license that can be easily used without any knowledge about GPU programming and we show that it is about 100 times faster than other state of the art CPU libraries. It may become a powerful tool for many applications that need to perform a large number of acoustic simulations, such as training machine learning systems for audio signal processing, or for real-time room acoustics simulations for immersive multimedia systems, such as augmented or virtual reality.},
	number = {4},
	urldate = {2024-10-14},
	journal = {Multimedia Tools and Applications},
	author = {Diaz-Guerra, David and Miguel, Antonio and Beltran, Jose R.},
	month = feb,
	year = {2021},
	note = {arXiv:1810.11359 [eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {5653--5671},
	annote = {Comment: This is a pre-print of an article published in Multimedia Tools and Applications (2020)},
}

@inproceedings{komatsu_sound_2021,
	address = {Amsterdam, Netherlands},
	title = {Sound {Event} {Localization} and {Detection} {Using} {Convolutional} {Recurrent} {Neural} {Networks} and {Gated} {Linear} {Units}},
	isbn = {978-90-827970-5-3},
	doi = {10.23919/Eusipco47968.2020.9287372},
	urldate = {2024-10-14},
	booktitle = {2020 28th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {IEEE},
	author = {Komatsu, Tatsuya and Togami, Masahito and Takahashi, Tsubasa},
	month = jan,
	year = {2021},
	pages = {41--45},
}

@article{lan_attention_2022,
	title = {Attention mechanism combined with residual recurrent neural network for sound event detection and localization},
	volume = {2022},
	issn = {1687-4722},
	doi = {10.1186/s13636-022-00263-6},
	abstract = {Abstract
            In the task of sound event detection and localization (SEDL) in a complex environment, the acoustic signals of different events usually have nonlinear superposition, so the detection and localization effect is not good. Given this, this paper is based on the Residual-spatially and channel Squeeze-Excitation (Res-scSE) model. Combined with Multiple-scale Convolutional Recurrent Neural Network (M-CRNN), the Res-scSE-CRNN model is proposed. Firstly, to solve the problem of insufficient extraction of time-frequency feature in single-size convolution kernel, multi-scale feature fusion is carried out by using the feature hierarchy of the convolutional neural network to improve the accuracy of detection. Secondly, aiming at the problem of overlapping audio event localization accuracy is not high, with Res-scSE to replace common convolution module and add residual structure to strengthen the feature extraction, and combining with an attention mechanism to enhance neural network channels and spatial relationships, to improve the network to extract the characteristics of directivity, achieve the goal of the overlapped audio localization. In this paper, experiments are carried out in the open dataset DCASE2019, and evaluation indicators are used to analyze the effectiveness of the proposed model and baseline model in the detection and localization of audio events. The results show that compared with the M-CRNN model, the detection error rate of Res-scSE-CRNN model is reduced 4\%, the F1-Score is increased 3.4\%, the localization error is reduced by 22.8°, and the frame recall rate is increased 3\%.},
	language = {en},
	number = {1},
	urldate = {2024-10-14},
	journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	author = {Lan, Chaofeng and Zhang, Lei and Zhang, Yuanyuan and Fu, Lirong and Sun, Chao and Han, Yulan and Zhang, Meng},
	month = dec,
	year = {2022},
	pages = {29},
}

@article{shin_seld_2023,
	title = {{SELD} {U}-{Net}: {Joint} {Optimization} of {Sound} {Event} {Localization} and {Detection} {With} {Noise} {Reduction}},
	volume = {11},
	issn = {2169-3536},
	shorttitle = {{SELD} {U}-{Net}},
	doi = {10.1109/ACCESS.2023.3318322},
	urldate = {2024-10-14},
	journal = {IEEE Access},
	author = {Shin, Yeongseo and Kim, Yong Guk and Choi, Chang-Ho and Kim, Dae-Joong and Chun, Chanjun},
	year = {2023},
	pages = {105379--105393},
}


@ARTICLE{malioutov_sparse_2005,
  author={Malioutov, D. and Cetin, M. and Willsky, A.S.},
  journal={IEEE Transactions on Signal Processing}, 
  title={A sparse signal reconstruction perspective for source localization with sensor arrays}, 
  year={2005},
  volume={53},
  number={8},
  pages={3010-3022},
  keywords={Sensor arrays;Signal reconstruction;Spatial resolution;Signal resolution;Array signal processing;Multiple signal classification;Maximum likelihood estimation;Acoustic sensors;Signal representations;Singular value decomposition;Direction-of-arrival estimation;overcomplete representation;sensor array processing;source localization;sparse representation;superresolution},
  doi={10.1109/TSP.2005.850882}}

@ARTICLE{limitation_MUSIC_2021,
  author={Zhagypar, Ruslan and Zhagyparova, Kalamkas and Akhtar, Muhammad Tahir},
  journal={IEEE Access}, 
  title={Spatially Smoothed TF-Root-MUSIC for DOA Estimation of Coherent and Non-Stationary Sources Under Noisy Conditions}, 
  year={2021},
  volume={9},
  number={},
  pages={95754-95766},
  keywords={Direction-of-arrival estimation;Estimation;Covariance matrices;Multiple signal classification;Time-frequency analysis;Smoothing methods;Eigenvalues and eigenfunctions;Direction-of-arrival estimation;MUSIC;Root-MUSIC;TF-MUSIC;sound source localization},
  doi={10.1109/ACCESS.2021.3095345}}

@article{westervelt_parametric_1963,
	title = {Parametric {Acoustic} {Array}},
	volume = {35},
	issn = {0001-4966},
	doi = {10.1121/1.1918525},
	abstract = {This paper presents the theory of highly directional receivers and transmitters that may be “constructed” with the nonlinearity of the equations of fluid motion.},
	number = {4},
	journal = {The Journal of the Acoustical Society of America},
	author = {Westervelt, Peter J.},
	month = apr,
	year = {1963},
	pages = {535--537},
}